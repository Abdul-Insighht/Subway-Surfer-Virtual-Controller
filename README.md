# Gesture Controlled Subway Surfers (Computer Vision)

A real-time **gesture-based virtual controller** for Subway Surfers built using **Computer Vision**.  
This project allows controlling the game using **hand movements and gestures**, without any machine learning models.

---

## ğŸš€ Features

- ğŸ– Open palm â†’ **Jump**
- ğŸ¤™ Thumb + Pinky â†’ **Slide**
- ğŸ‘ˆ Hand on left side â†’ **Move Left**
- ğŸ‘‰ Hand on right side â†’ **Move Right**
- ğŸ“Š Live FPS & action display
- ğŸ§  Stable, rule-based gesture logic (No ML)

---

## ğŸ›  Tech Stack

- Python
- OpenCV
- MediaPipe
- Pynput

---

## âš™ï¸ How It Works

- MediaPipe detects hand landmarks in real-time
- Hand position determines left / right lane movement
- Specific finger combinations trigger jump & slide
- Keyboard inputs are simulated using `pynput`

This approach focuses on **stability, low latency, and simplicity**, making it suitable for real-time gameplay.

---

## â–¶ï¸ How to Run

1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/gesture-controlled-subway-surfers.git
